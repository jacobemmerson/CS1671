{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS1671 A3 Scratch\n",
    "### Jacob Emmerson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. Evaluation Metrics ####\n",
    "\n",
    "## Input: y_pred, a list of length n with the predicted labels,\n",
    "## y_true, a list of length n with the true labels\n",
    "\n",
    "## Calculates the precision of the predicted labels\n",
    "def get_precision(y_pred, y_true):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for p in range(len(y_pred)): # len(pred) == len(true); p = prediction index\n",
    "        if y_pred[p] == 1: # is it PREDICTED positive\n",
    "            if y_true[p] == 1: # correct prediction\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "    return (tp/(tp+fp))\n",
    "    \n",
    "## Calculates the recall of the predicted labels\n",
    "def get_recall(y_pred, y_true):\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "\n",
    "    for p in range(len(y_true)):\n",
    "        if y_true[p] == 1: # check all positive samples\n",
    "            if y_pred[p] == 1: # correct prediction\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "    return (tp/(tp+fn)) # recall == sensitivity\n",
    "\n",
    "## Calculates the f-score of the predicted labels\n",
    "def get_fscore(y_pred, y_true):\n",
    "    p = get_precision(y_pred, y_true)\n",
    "    r = get_recall(y_pred, y_true)\n",
    "\n",
    "    fscore = 2 * ((p * r)/(p + r))\n",
    "\n",
    "    return fscore\n",
    "\n",
    "def test_predictions(y_pred, y_true):\n",
    "\n",
    "    f = get_fscore(y_pred, y_true)\n",
    "    p = get_precision(y_pred, y_true)\n",
    "    r = get_recall(y_pred, y_true)\n",
    "\n",
    "    print(f\"Precision = {p}\")\n",
    "    print(f\"Recall = {r}\")\n",
    "    print(f\"F-Score = {f}\")\n",
    "\n",
    "    return (p,r,f) # returns a tuple of precision, recall, and fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2. Complex Word Identification ####\n",
    "\n",
    "## Loads in the words and labels of one of the datasets\n",
    "def load_file(data_file):\n",
    "    words = []\n",
    "    labels = []   \n",
    "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                words.append(line_split[0].lower())\n",
    "                labels.append(int(line_split[1]))\n",
    "            i += 1\n",
    "    return words, labels\n",
    "\n",
    "### 2.1: A very simple baseline\n",
    "\n",
    "## Makes feature matrix for all complex\n",
    "def all_complex_feature(words):\n",
    "    return [words,[1] * len(words)]\n",
    "\n",
    "## Labels every word complex\n",
    "def all_complex(data_file):\n",
    "    data = load_file(data_file)\n",
    "    words = data[0] # data is a tuple where data_file[0] are the words and [1] are the labels\n",
    "    true_labels = data[1]\n",
    "    preds = all_complex_feature(words)[1]\n",
    "    p,r,f = test_predictions(preds, true_labels)\n",
    "\n",
    "    performance = [p, r, f]\n",
    "    return performance\n",
    "\n",
    "### 2.2: Word length thresholding\n",
    "\n",
    "## Makes feature matrix for word_length_threshold\n",
    "def length_threshold_feature(words, threshold):\n",
    "    preds = []\n",
    "    for w in words:\n",
    "        if len(w) >= threshold:\n",
    "            preds.append(1)\n",
    "        else:\n",
    "            preds.append(0)\n",
    "    return [words,preds]\n",
    "\n",
    "## Finds the best length threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "def word_length_threshold(training_file, development_file):\n",
    "    thresh = 0\n",
    "    t_data = load_file(training_file)\n",
    "    t_words = t_data[0]\n",
    "    t_labels = t_data[1]\n",
    "\n",
    "    d_data = load_file(development_file)\n",
    "    d_words = d_data[0]\n",
    "    d_labels = d_data[1]\n",
    "\n",
    "    best_f = 0\n",
    "    for t in range(21): # try thresh [0,20]\n",
    "        temp = length_threshold_feature(t_words, t)\n",
    "        f = get_fscore(temp[1], t_labels)\n",
    "        if f > best_f:\n",
    "            best_f = f\n",
    "            thresh = t\n",
    "\n",
    "    print(f\"Best Threshold Found = {thresh}\")\n",
    "    print('-' * 20)\n",
    "    \n",
    "    t_M = length_threshold_feature(t_words, thresh)\n",
    "    d_M = length_threshold_feature(d_words, thresh)\n",
    "\n",
    "    print(\"Training Scores:\")\n",
    "    tp, tr, tf = test_predictions(t_M[1], t_labels)\n",
    "\n",
    "    print(\"\\nDevelopment Scores:\")\n",
    "    dp, dr, df = test_predictions(d_M[1], d_labels)\n",
    "\n",
    "    training_performance = [tp, tr, tf]\n",
    "    development_performance = [dp, dr, df]\n",
    "    return training_performance, development_performance\n",
    "\n",
    "### 2.3: Word frequency thresholding\n",
    "\n",
    "## Loads Google NGram counts\n",
    "def load_ngram_counts(ngram_counts_file): \n",
    "   counts = defaultdict(int) \n",
    "   with gzip.open(ngram_counts_file, 'rt') as f: \n",
    "       for line in f:\n",
    "           token, count = line.strip().split('\\t') \n",
    "           if token[0].islower(): \n",
    "               counts[token] = int(count) \n",
    "   return counts\n",
    "\n",
    "# Finds the best frequency threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "\n",
    "## Make feature matrix for word_frequency_threshold\n",
    "def frequency_threshold_feature(words, threshold, counts):\n",
    "    preds = []\n",
    "    for w in words:\n",
    "        try: # catch unseen words\n",
    "            freq = counts[w]\n",
    "        except:\n",
    "            freq = 0 \n",
    "\n",
    "        if freq <= threshold: # if word is not frequently seen, it is complex\n",
    "            preds.append(1)\n",
    "        else:\n",
    "            preds.append(0)\n",
    "\n",
    "    return [words,preds]\n",
    "\n",
    "def word_frequency_threshold(training_file, development_file, counts):\n",
    "    thresh = 0\n",
    "    t_data = load_file(training_file)\n",
    "    t_words = t_data[0]\n",
    "    t_labels = t_data[1]\n",
    "\n",
    "    d_data = load_file(development_file)\n",
    "    d_words = d_data[0]\n",
    "    d_labels = d_data[1]\n",
    "\n",
    "    # bounds for threshold optimization\n",
    "    l_B = min(counts.values())\n",
    "    u_B = max(counts.values()) // 2 # divison by 2 is optional (a thresh = upper bound would be equal to first baseline)\n",
    "\n",
    "    print(f\"Trying Thresholds between {[l_B, u_B]}\")\n",
    "\n",
    "    best_f = 0\n",
    "    for t in np.linspace(l_B, u_B, 100000): # try 100000 thresholds\n",
    "        temp = frequency_threshold_feature(t_words, threshold = t, counts = counts)\n",
    "        f = get_fscore(temp[1], t_labels) \n",
    "        if f > best_f:\n",
    "            best_f = f\n",
    "            thresh = t\n",
    "        else: break # quick stopping, assumes local max = global max\n",
    "\n",
    "    print(f\"Best Threshold Found = {thresh}\")\n",
    "    print('-' * 20)\n",
    "    t_M = frequency_threshold_feature(t_words, threshold = thresh, counts = counts)\n",
    "    d_M = frequency_threshold_feature(d_words, threshold = thresh, counts = counts)\n",
    "\n",
    "    print(\"Training Scores:\")\n",
    "    tp, tr, tf = test_predictions(t_M[1], t_labels)\n",
    "\n",
    "    print(\"\\nDevelopment Scores:\")\n",
    "    dp, dr, df = test_predictions(d_M[1], d_labels)\n",
    "\n",
    "    training_performance = [tp, tr, tf]\n",
    "    development_performance = [dp, dr, df]\n",
    "    return training_performance, development_performance\n",
    "\n",
    "### 2.4: Naive Bayes\n",
    "        \n",
    "## Trains a Naive Bayes classifier using length and frequency features\n",
    "def naive_bayes(training_file, development_file, counts):\n",
    "    ## YOUR CODE HERE\n",
    "    training_performance = (tprecision, trecall, tfscore)\n",
    "    development_performance = (dprecision, drecall, dfscore)\n",
    "    return development_performance\n",
    "\n",
    "### 2.5: Logistic Regression\n",
    "\n",
    "## Trains a Naive Bayes classifier using length and frequency features\n",
    "def logistic_regression(training_file, development_file, counts):\n",
    "    ## YOUR CODE HERE    \n",
    "    training_performance = (tprecision, trecall, tfscore)\n",
    "    development_performance = (dprecision, drecall, dfscore)\n",
    "    return development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = \"data/complex_words_training.txt\"\n",
    "development_file = \"data/complex_words_development.txt\"\n",
    "test_file = \"data/complex_words_test_unlabeled.txt\"\n",
    "\n",
    "train_data = load_file(training_file)\n",
    "\n",
    "ngram_counts_file = \"ngram_counts.txt.gz\"\n",
    "counts = load_ngram_counts(ngram_counts_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.43275\n",
      "Recall = 1.0\n",
      "F-Score = 0.604083057058105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.43275, 1.0, 0.604083057058105]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_complex(training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.418\n",
      "Recall = 1.0\n",
      "F-Score = 0.5895627644569816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.418, 1.0, 0.5895627644569816]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_complex(development_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold Found = 7\n",
      "--------------------\n",
      "Training Scores:\n",
      "Precision = 0.6007401315789473\n",
      "Recall = 0.8440207972270364\n",
      "F-Score = 0.7018976699495555\n",
      "\n",
      "Development Scores:\n",
      "Precision = 0.6053511705685619\n",
      "Recall = 0.8660287081339713\n",
      "F-Score = 0.7125984251968505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6007401315789473, 0.8440207972270364, 0.7018976699495555],\n",
       " [0.6053511705685619, 0.8660287081339713, 0.7125984251968505])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_length_threshold(training_file, development_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying Thresholds between [0, 23688414825]\n",
      "Best Threshold Found = 10896779.787297873\n",
      "--------------------\n",
      "Training Scores:\n",
      "Precision = 0.5999008428358948\n",
      "Recall = 0.6990179087232813\n",
      "F-Score = 0.6456776947705442\n",
      "\n",
      "Development Scores:\n",
      "Precision = 0.603515625\n",
      "Recall = 0.7392344497607656\n",
      "F-Score = 0.664516129032258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5999008428358948, 0.6990179087232813, 0.6456776947705442],\n",
       " [0.603515625, 0.7392344497607656, 0.664516129032258])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency_threshold(training_file, development_file, counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
