{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS1671 Assignment 4: Vector Space Models\n",
    "### Jacob Emmerson\n",
    "Due: Novemeber 20th, 2023 @ 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment primarily focuses arround the material from chapter 6 in *Speech and Language Processing* (3rd Ed.)\n",
    "\n",
    "**Primary Question:** How good are vector space representations built using Shakespeare data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hw4_skeleton_jte27 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://numpy.org/doc/stable/reference/routines.array-creation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_shakespeare():\n",
    "    \"\"\"Reads in the Shakespeare dataset processesit into a list of tuples.\n",
    "       Also reads in the vocab and play name lists from files.\n",
    "\n",
    "    Each tuple consists of\n",
    "    tuple[0]: The name of the play\n",
    "    tuple[1] A line from the play as a list of tokenized words.\n",
    "\n",
    "    Returns:\n",
    "      tuples: A list of tuples in the above format.\n",
    "      document_names: A list of the plays present in the corpus.\n",
    "      vocab: A list of all tokens in the vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    tuples = []\n",
    "\n",
    "    with open(\"will_play_text.csv\") as f:\n",
    "        csv_reader = csv.reader(f, delimiter=\";\")\n",
    "        for row in csv_reader:\n",
    "            play_name = row[1]\n",
    "            line = row[5]\n",
    "            line_tokens = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", line).split()\n",
    "            line_tokens = [token.lower() for token in line_tokens]\n",
    "\n",
    "            tuples.append((play_name, line_tokens))\n",
    "\n",
    "    with open(\"vocab.txt\") as f:\n",
    "        vocab = [line.strip() for line in f]\n",
    "\n",
    "    with open(\"play_names.txt\") as f:\n",
    "        document_names = [line.strip() for line in f]\n",
    "\n",
    "    return tuples, document_names, vocab\n",
    "\n",
    "\n",
    "def get_row_vector(matrix, row_id):\n",
    "    \"\"\"A convenience function to get a particular row vector from a numpy matrix\n",
    "\n",
    "    Inputs:\n",
    "      matrix: a 2-dimensional numpy array\n",
    "      row_id: an integer row_index for the desired row vector\n",
    "\n",
    "    Returns:\n",
    "      1-dimensional numpy array of the row vector\n",
    "    \"\"\"\n",
    "    return matrix[row_id, :]\n",
    "\n",
    "\n",
    "def get_column_vector(matrix, col_id):\n",
    "    \"\"\"A convenience function to get a particular column vector from a numpy matrix\n",
    "\n",
    "    Inputs:\n",
    "      matrix: a 2-dimensional numpy array\n",
    "      col_id: an integer col_index for the desired row vector\n",
    "\n",
    "    Returns:\n",
    "      1-dimensional numpy array of the column vector\n",
    "    \"\"\"\n",
    "    return matrix[:, col_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_document_matrix(line_tuples, document_names, vocab):\n",
    "    \"\"\"Returns a numpy array containing the term document matrix for the input lines.\n",
    "\n",
    "    Inputs:\n",
    "      line_tuples: A list of tuples, containing the name of the document and\n",
    "      a tokenized line from that document.\n",
    "      document_names: A list of the document names\n",
    "      vocab: A list of the tokens in the vocabulary\n",
    "\n",
    "    # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:39 PM.\n",
    "\n",
    "    Let m = len(vocab) and n = len(document_names).\n",
    "\n",
    "    Returns:\n",
    "      td_matrix: A mxn numpy array where the number of rows is the number of words\n",
    "          and each column corresponds to a document. A_ij contains the\n",
    "          frequency with which word i occurs in document j.\n",
    "    \"\"\"\n",
    "    m = len(vocab)\n",
    "    n = len(document_names)\n",
    "\n",
    "    doc_index = dict(zip(document_names, range(len(document_names))))\n",
    "    word_index = dict(zip(vocab, range(len(vocab))))\n",
    "    td_matrix = np.zeros(shape = (m,n), dtype=np.int32)\n",
    "\n",
    "    for line in line_tuples:\n",
    "        di = doc_index[line[0]]\n",
    "        words = line[1]\n",
    "        for w in words:\n",
    "            wi = word_index[w]\n",
    "            td_matrix[wi, di] += 1\n",
    "\n",
    "    return td_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_context_matrix(line_tuples, vocab, context_window_size=1):\n",
    "    \"\"\"Returns a numpy array containing the term context matrix for the input lines.\n",
    "\n",
    "    Inputs:\n",
    "      line_tuples: A list of tuples, containing the name of the document and\n",
    "      a tokenized line from that document.\n",
    "      vocab: A list of the tokens in the vocabulary\n",
    "\n",
    "    # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:39 PM.\n",
    "\n",
    "    Let n = len(vocab).\n",
    "\n",
    "    Returns:\n",
    "      tc_matrix: A nxn numpy array where A_ij contains the frequency with which\n",
    "          word j was found within context_window_size to the left or right of\n",
    "          word i in any sentence in the tuples.\n",
    "    \"\"\"\n",
    "    n = len(vocab)\n",
    "    cws = context_window_size\n",
    "\n",
    "    word_index = dict(zip(vocab, range(len(vocab))))\n",
    "    tc_matrix = np.zeros(shape = (n,n), dtype = np.int32)\n",
    "\n",
    "\n",
    "    for line in line_tuples:\n",
    "        words = line[1]\n",
    "\n",
    "        for i in range(len(words)):\n",
    "            wi = word_index[words[i]] # target word (row index)\n",
    "            \n",
    "            L_win = words[max(0, i - cws):(i)] # upper is exclusive\n",
    "            if i == len(words): # if we are at the end of the sentence, no upper window \n",
    "              #(i + 1) throws an error\n",
    "                U_win = []\n",
    "            else: \n",
    "                U_win = words[(i+1):(i + cws + 1)] # don't include the target word\n",
    "\n",
    "            window = L_win + U_win\n",
    "            for word in window: # add the word instances to the tc_matrix\n",
    "                wj = word_index[word] #context index\n",
    "                tc_matrix[wi,wj] += 1\n",
    "\n",
    "    return tc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_idf_matrix(term_document_matrix):\n",
    "    \"\"\"Given the term document matrix, output a tf-idf weighted version.\n",
    "\n",
    "    See section 6.5 in the textbook.\n",
    "\n",
    "    Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "\n",
    "    Input:\n",
    "      term_document_matrix: Numpy array where each column represents a document\n",
    "      and each row, the frequency of a word in that document.\n",
    "\n",
    "    Returns:\n",
    "      A numpy array with the same dimension as term_document_matrix, where\n",
    "      A_ij is weighted by the inverse document frequency of document h.\n",
    "    \"\"\"\n",
    "    N = term_document_matrix.shape[1] # number of documents\n",
    "    tf = np.log(term_document_matrix + 1) # log counts of term frequencies\n",
    "    df = np.count_nonzero(term_document_matrix, axis = 1)\n",
    "    idf = np.log(N/df)\n",
    "    \n",
    "    for d in range(N):\n",
    "       tf[:,d] *= idf \n",
    "    \n",
    "    return tf #tf-idf mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ppmi_matrix(term_context_matrix):\n",
    "    \"\"\"Given the term context matrix, output a ppmi weighted version.\n",
    "\n",
    "    See section 6.6 in the textbook.\n",
    "\n",
    "    Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "\n",
    "    Input:\n",
    "      term_context_matrix: Numpy array where each cell represents whether the \n",
    "\t  word in the row appears within a window of the word in the column.\n",
    "\n",
    "    Returns:\n",
    "      A numpy array with the same dimension as term_context_matrix, where\n",
    "      A_ij is weighted using PPMI.\n",
    "    \"\"\"\n",
    "    word_counts = term_context_matrix.sum(axis = 1) #rows\n",
    "    context_counts = term_context_matrix.sum(axis = 0) #columns\n",
    "    total = term_context_matrix.sum() # total matrix counts\n",
    "    word_prob = word_counts / total\n",
    "    context_prob = context_counts / total\n",
    "\n",
    "    tcm = term_context_matrix / total # joint probabilities\n",
    "    tcm = tcm / context_prob \n",
    "    tcm = tcm / word_prob[:,np.newaxis]\n",
    "    tcm = np.log2(tcm)\n",
    "\n",
    "    tcm[tcm < 0] = 0 # replace all negative values\n",
    "    \n",
    "    return tcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(vector1, vector2):\n",
    "    \"\"\"Computes the cosine similarity of the two input vectors.\n",
    "\n",
    "    Inputs:\n",
    "      vector1: A nx1 numpy array\n",
    "      vector2: A nx1 numpy array\n",
    "\n",
    "    Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "      A scalar similarity value.\n",
    "    \"\"\"\n",
    "\n",
    "    len1 = np.sqrt(vector1.dot(vector1))\n",
    "    len2 = np.sqrt(vector2.dot(vector2))\n",
    "\n",
    "    if len1 == 0 or len2 == 0: # prevent divison by zero; caused by frequent terms using tf-idf or infrequent words in tc matrix\n",
    "        return 0\n",
    "\n",
    "    return vector1.dot(vector2)/(len1 * len2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_column(matrix, col_index, ascending = False):\n",
    "    mat = matrix\n",
    "    if ascending:\n",
    "        a = 1\n",
    "    else:\n",
    "        a = -1\n",
    "    # uses quicksort (not stable), non issue with continuous data\n",
    "    return mat[mat[:,col_index].argsort(kind = \"quicksort\")[::a]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_words(target_word_index, matrix):\n",
    "    \"\"\"Ranks the similarity of all of the words to the target word using compute_cosine_similarity.\n",
    "\n",
    "    Inputs:\n",
    "      target_word_index: The index of the word we want to compare all others against.\n",
    "      matrix: Numpy matrix where the ith row represents a vector embedding of the ith word.\n",
    "\n",
    "    Returns:\n",
    "      A length-n list of integer word indices, ordered by decreasing similarity to the\n",
    "      target word indexed by word_index\n",
    "      A length-n list of similarity scores, ordered by decreasing similarity to the\n",
    "      target word indexed by word_index\n",
    "    \"\"\"\n",
    "    mat = matrix\n",
    "    nrows = mat.shape[0] # number of rows  \n",
    "    woi = mat[target_word_index,:] # word of interest  \n",
    "\n",
    "    for r in range(nrows):\n",
    "        if r == target_word_index: # don't compute similarity with itself\n",
    "            continue\n",
    "        \n",
    "        s = compute_cosine_similarity(woi, mat[r,:])\n",
    "\n",
    "        # create the array on first pass\n",
    "        if r == 0:\n",
    "            sims = np.array([[r,s]])\n",
    "\n",
    "        else:\n",
    "          sims = np.append(sims, [[r,s]], axis = 0)\n",
    "    \n",
    "    sims = sort_by_column(sims, 1)\n",
    "\n",
    "    return sims[:,0].astype(np.int32), sims[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing term document matrix...\n",
      "Computing tf-idf matrix...\n",
      "Computing term context matrix...\n",
      "\n",
      "The 10 most similar words to \"juliet\" using cosine-similarity on term-document frequency matrix are:\n",
      "1: capulet; 0.9899494936611666\n",
      "2: heartless; 0.9899494936611665\n",
      "3: tormented; 0.9899494936611665\n",
      "4: festering; 0.9899494936611665\n",
      "5: caetera; 0.9899494936611665\n",
      "6: baptized; 0.9899494936611665\n",
      "7: bescreen; 0.9899494936611665\n",
      "8: dissemblers; 0.9899494936611665\n",
      "9: goeth; 0.9899494936611665\n",
      "10: overwhelming; 0.9899494936611665\n",
      "\n",
      "The 10 most similar words to \"juliet\" using cosine-similarity on term-context frequency matrix are:\n",
      "1: warwick; 0.7573818930001273\n",
      "2: lucius; 0.7572798806352675\n",
      "3: gloucester; 0.732453904665193\n",
      "4: antonio; 0.7257120649096317\n",
      "5: helena; 0.7222425324074834\n",
      "6: othello; 0.7215767990471125\n",
      "7: servants; 0.7215132676180231\n",
      "8: brutus; 0.712404967855317\n",
      "9: claudio; 0.7088029472697824\n",
      "10: clifford; 0.7004681194274702\n",
      "\n",
      "The 10 most similar words to \"juliet\" using cosine-similarity on tf-idf matrix are:\n",
      "1: procures; 0.9611235625180706\n",
      "2: benedicite; 0.9611235625180706\n",
      "3: ghostly; 0.9380710616293596\n",
      "4: mercutio; 0.8748623439660793\n",
      "5: montagues; 0.8748623439660793\n",
      "6: smatter; 0.8748623439660792\n",
      "7: drivelling; 0.8748623439660792\n",
      "8: beguil; 0.8748623439660792\n",
      "9: grubs; 0.8748623439660792\n",
      "10: duellist; 0.8748623439660792\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tuples, document_names, vocab = read_in_shakespeare()\n",
    "\n",
    "    print(\"Computing term document matrix...\")\n",
    "    td_matrix = create_term_document_matrix(tuples, document_names, vocab)\n",
    "\n",
    "    print(\"Computing tf-idf matrix...\")\n",
    "    tf_idf_matrix = create_tf_idf_matrix(td_matrix)\n",
    "\n",
    "\n",
    "    print(\"Computing term context matrix...\")\n",
    "    tc_matrix = create_term_context_matrix(tuples, vocab, context_window_size=2)\n",
    "\n",
    "    # random_idx = random.randint(0, len(document_names) - 1)\n",
    "\n",
    "    word = \"juliet\"\n",
    "    vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "\n",
    "    print(\n",
    "        '\\nThe 10 most similar words to \"%s\" using cosine-similarity on term-document frequency matrix are:'\n",
    "        % (word)\n",
    "    )\n",
    "    ranks, scores = rank_words(vocab_to_index[word], td_matrix)\n",
    "    for idx in range(0,10):\n",
    "        word_id = ranks[idx]\n",
    "        print(\"%d: %s; %s\" %(idx+1, vocab[word_id], scores[idx]))\n",
    "\n",
    "    print(\n",
    "        '\\nThe 10 most similar words to \"%s\" using cosine-similarity on term-context frequency matrix are:'\n",
    "        % (word)\n",
    "    )\n",
    "    ranks, scores = rank_words(vocab_to_index[word], tc_matrix)\n",
    "    for idx in range(0,10):\n",
    "        word_id = ranks[idx]\n",
    "        print(\"%d: %s; %s\" %(idx+1, vocab[word_id], scores[idx]))\n",
    "\n",
    "\n",
    "    print(\n",
    "        '\\nThe 10 most similar words to \"%s\" using cosine-similarity on tf-idf matrix are:'\n",
    "        % (word)\n",
    "    )\n",
    "    ranks, scores = rank_words(vocab_to_index[word], tf_idf_matrix)\n",
    "    for idx in range(0,10):\n",
    "        word_id = ranks[idx]\n",
    "        print(\"%d: %s; %s\" %(idx+1, vocab[word_id], scores[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt,dn,v = read_in_shakespeare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = create_term_document_matrix(lt,dn,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = create_term_context_matrix(lt, v, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = create_tf_idf_matrix(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "create_ppmi_matrix(tc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
