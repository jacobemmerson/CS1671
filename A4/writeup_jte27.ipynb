{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS1671 Assignment 4: Vector Space Models\n",
    "### Jacob Emmerson\n",
    "Due: Novemeber 20th, 2023 @ 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment primarily focuses arround the material from chapter 6 in *Speech and Language Processing* (3rd Ed.)\n",
    "\n",
    "**Primary Question:** How good are vector space representations built using Shakespeare data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sources*\n",
    "- https://numpy.org/doc/stable/reference/routines.array-creation.html (NumPy Documentation for Array Initialization)\n",
    "- https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html (Compressed Sparse Rows for Memory Optimization during PPMI calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw4_skeleton_jte27 import *\n",
    "from scipy.stats import kendalltau\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "def get_ranks(matrix, words, vocab, vocab_to_index):\n",
    "    rank_df = pd.DataFrame({'rank' : range(1,11)}).set_index('rank')\n",
    "    for word in words:\n",
    "        ranks, scores = rank_words(vocab_to_index[word], matrix)\n",
    "        t10_words = [vocab[r] for r in ranks[:10]]\n",
    "        t10_scores = np.round(scores[:10], 4)\n",
    "\n",
    "        rank_df[word] = tuple(zip(t10_words,t10_scores))\n",
    "    return rank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limited memory on my machine (WSL2, Max 6GB of Memory with 4 Processors), I subset the data. \n",
    "\n",
    "The subset of data will be used for each experiment. This technique works by selecting either the first $N$ documents or randomly sampling $N$ documents from the given document list. To maintain reproducibility, I use the first $N$ documents. Additionally, the vocabulary is subsetted to only the words that occur within the document subset to reduce memory usage when computing PPMI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, dn, v = read_in_shakespeare()\n",
    "tuples, document_names, vocab = subset_data(t, dn, 12, random = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original |V| = 22602\n",
      "Subset |V| = 14592\n",
      "8010 words not included.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original |V| = {len(v)}\")\n",
    "print(f\"Subset |V| = {len(vocab)}\")\n",
    "print(f\"{len(v) - len(vocab)} words not included.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following documents are used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Henry IV',\n",
       " 'Alls well that ends well',\n",
       " 'Loves Labours Lost',\n",
       " 'Taming of the Shrew',\n",
       " 'Antony and Cleopatra',\n",
       " 'Coriolanus',\n",
       " 'Hamlet',\n",
       " 'A Midsummer nights dream',\n",
       " 'Merry Wives of Windsor',\n",
       " 'Romeo and Juliet',\n",
       " 'Richard II',\n",
       " 'King John']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Frequency Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing term document matrix...\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing term document matrix...\")\n",
    "td_matrix = create_term_document_matrix(tuples, document_names, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing term context matrix...\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing term context matrix...\")\n",
    "tc_matrix = create_term_context_matrix(tuples, vocab, context_window_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Can a word co-occur with itself in a Term-Context Matrix? \n",
    "\n",
    "While it is possible to allow a word to co-occur with itself in a term-context matrix, I do not permit this to happen. The goal of a term-context matrix is to represent the relationships between a target word and all other words, so the inclusion of the word's occurence with itself is not necessary.\n",
    "\n",
    "This circumstance changes if the target happens to be within the context window of itself. An instance of this would be a sentence like \"He is ***very*** *very* excited\" where the target word is emphasized. This example is unlikely to occur in one of Shakespeare's plays; however, it demonstrates an occurence in which a target word may occur with itself. With a large context window, we may see words such as \"the\" or to-be verbs occuring with themselves.\n",
    "\n",
    "Additionally, since we are using cosine similarity as our metric of interest for our experiments, we would not want our frequencies for more common words to be inflated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctcm(line_tuples, vocab, context_window_size=1):\n",
    "    \"\"\"Returns a numpy array containing the term context matrix for the input lines.\n",
    "\n",
    "    Inputs:\n",
    "      line_tuples: A list of tuples, containing the name of the document and\n",
    "      a tokenized line from that document.\n",
    "      vocab: A list of the tokens in the vocabulary\n",
    "\n",
    "    # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:39 PM.\n",
    "\n",
    "    Let n = len(vocab).\n",
    "\n",
    "    Returns:\n",
    "      tc_matrix: A nxn numpy array where A_ij contains the frequency with which\n",
    "          word j was found within context_window_size to the left or right of\n",
    "          word i in any sentence in the tuples.\n",
    "    \"\"\"\n",
    "    n = len(vocab)\n",
    "    cws = context_window_size\n",
    "\n",
    "    word_index = dict(zip(vocab, range(len(vocab))))\n",
    "    tc_matrix = np.zeros(shape = (n,n), dtype = np.int32)\n",
    "\n",
    "    for line in line_tuples:\n",
    "        sentence = line[1] # don't care about documents\n",
    "\n",
    "        for i in range(len(sentence)):\n",
    "            target_index = word_index[sentence[i]] # target word (row index)\n",
    "            \n",
    "            L_win = sentence[max(0, i - cws):(i)] # upper is exclusive\n",
    "            if i == len(sentence): # if we are at the end of the sentence, no upper window \n",
    "              #(i + 1) throws an error\n",
    "                U_win = []\n",
    "            else: \n",
    "                U_win = sentence[(i):(i + cws + 1)] \n",
    "\n",
    "            window = L_win + U_win\n",
    "            for word in window: # add the word instances to the tc_matrix\n",
    "                wj = word_index[word] #context index\n",
    "                tc_matrix[target_index,wj] += 1\n",
    "\n",
    "    return tc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_tc = ctcm(tuples, vocab, context_window_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A version of `create_term_context_matrix` with self-inclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 10 Similar Words using a Self-Inclusive Term Context Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>juliet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(pined, 0.5597)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(waken, 0.4198)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(and, 0.2526)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(drinkings, 0.2021)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(scamble, 0.1999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(metheglins, 0.1999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(wills, 0.1941)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(bleeding, 0.1933)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(wakes, 0.1871)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(enter, 0.186)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    juliet\n",
       "rank                      \n",
       "1          (pined, 0.5597)\n",
       "2          (waken, 0.4198)\n",
       "3            (and, 0.2526)\n",
       "4      (drinkings, 0.2021)\n",
       "5        (scamble, 0.1999)\n",
       "6     (metheglins, 0.1999)\n",
       "7          (wills, 0.1941)\n",
       "8       (bleeding, 0.1933)\n",
       "9          (wakes, 0.1871)\n",
       "10          (enter, 0.186)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ranks(bad_tc, ['juliet'], vocab, vocab_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exclusive Term Context Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>juliet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(helena, 0.7085)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(therefore, 0.6935)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(nurse, 0.6871)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(demetrius, 0.6843)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(bertram, 0.681)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(costard, 0.6781)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(falstaff, 0.6771)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(caesar, 0.676)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(her, 0.6709)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(others, 0.6697)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   juliet\n",
       "rank                     \n",
       "1        (helena, 0.7085)\n",
       "2     (therefore, 0.6935)\n",
       "3         (nurse, 0.6871)\n",
       "4     (demetrius, 0.6843)\n",
       "5        (bertram, 0.681)\n",
       "6       (costard, 0.6781)\n",
       "7      (falstaff, 0.6771)\n",
       "8         (caesar, 0.676)\n",
       "9           (her, 0.6709)\n",
       "10       (others, 0.6697)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ranks(tc_matrix, ['juliet'], vocab, vocab_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our above ranks using the word 'juliet', we can see that a term-context matrix which allows for words to co-occure with themselves tends to rank irrelvant words much higher. We can see that these words tend to be more common such as \"and\" as well as \"enter\". Additionally, these words tend to have less of an intuitive reasoning for the similarity. \n",
    "\n",
    "With our exclusive term-context matrix, we see words ranked with a higher cosine similarity which additionally make more sense as being related to \"juliet\". Explanations for these ranks will be expanded on in **Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing tf-idf matrix...\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing tf-idf matrix...\")\n",
    "tf_idf_matrix = create_tf_idf_matrix(td_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save memory, I directly calculate the probabilities where possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $N$ denote the total number of counts in a $w \\times c$ term-context matrix \n",
    "\n",
    "\\begin{align*}\n",
    "PMI(w,c) &= \\log_2 \\frac{P(w,c)}{P(w)P(c)} \\\\\n",
    "         &= \\log_2 \\frac{counts(w,c)/N}{(counts(w)/N) \\cdot (counts(c)/N)} \\\\\n",
    "         &= \\log_2 \\frac{counts(w,c)}{(1/N)counts(w)counts(c)} \\\\\n",
    "         &= \\log_2 \\frac{N \\cdot counts(w,c)}{counts(w) counts(c)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPMI can now be efficiently calculated using vector arithmetic and numpy's methods for dealing with negative values and $nans$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing ppmi matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kibit/anaconda3/lib/python3.11/site-packages/scipy/sparse/_base.py:665: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.true_divide(self.todense(), other)\n",
      "/home/kibit/pitt/CS1671/A4/hw4_skeleton_jte27.py:183: RuntimeWarning: divide by zero encountered in log2\n",
      "  ppmi_mat = np.log2((tcm * total) / (word_counts * context_counts))\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing ppmi matrix...\")\n",
    "ppmi_matrix = create_ppmi_matrix(tc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ignore the runtime warnings, these are due to log(0) which are handled during the removal of negative PMI values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate each vector space model, I will find the top 10 most similar words to \n",
    "- juliet\n",
    "- romeo\n",
    "- royal\n",
    "- evil\n",
    "- wicked\n",
    "- he"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these rankings will likely change if the full dataset is used. These were computed using a subset of 12 documents from the 36 document pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['juliet', 'romeo', 'royal', 'evil', 'wicked', 'he']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_ranks` computes the most similar words using the `rank_words` function as outlined in the assignment guidelines and stores them into a dataframe where each column is our choice word and the rows are tuples of the highest ranked word with a cosine similarity score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-Document Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 10 most similar words to \"['juliet', 'romeo', 'royal', 'evil', 'wicked', 'he']\" using cosine-similarity on term-document frequency matrix are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>juliet</th>\n",
       "      <th>romeo</th>\n",
       "      <th>royal</th>\n",
       "      <th>evil</th>\n",
       "      <th>wicked</th>\n",
       "      <th>he</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(displant, 1.0)</td>\n",
       "      <td>(gossamer, 1.0)</td>\n",
       "      <td>(sovereign, 0.9676)</td>\n",
       "      <td>(suit, 0.922)</td>\n",
       "      <td>(offence, 0.9118)</td>\n",
       "      <td>(him, 0.9928)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(heretics, 1.0)</td>\n",
       "      <td>(dowdy, 1.0)</td>\n",
       "      <td>(forego, 0.9416)</td>\n",
       "      <td>(beam, 0.9152)</td>\n",
       "      <td>(question, 0.9032)</td>\n",
       "      <td>(have, 0.9894)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(shapen, 1.0)</td>\n",
       "      <td>(rooteth, 1.0)</td>\n",
       "      <td>(boast, 0.9395)</td>\n",
       "      <td>(employment, 0.9058)</td>\n",
       "      <td>(conscience, 0.9003)</td>\n",
       "      <td>(had, 0.9886)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(hopeful, 1.0)</td>\n",
       "      <td>(engrossing, 1.0)</td>\n",
       "      <td>(crown, 0.9385)</td>\n",
       "      <td>(foolery, 0.9014)</td>\n",
       "      <td>(image, 0.8997)</td>\n",
       "      <td>(cannot, 0.9874)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(tilts, 1.0)</td>\n",
       "      <td>(scaring, 1.0)</td>\n",
       "      <td>(highness, 0.9354)</td>\n",
       "      <td>(object, 0.887)</td>\n",
       "      <td>(top, 0.8986)</td>\n",
       "      <td>(would, 0.986)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(vowel, 1.0)</td>\n",
       "      <td>(carelessly, 1.0)</td>\n",
       "      <td>(burthen, 0.9301)</td>\n",
       "      <td>(rags, 0.8825)</td>\n",
       "      <td>(gifts, 0.8943)</td>\n",
       "      <td>(known, 0.9796)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(grievance, 1.0)</td>\n",
       "      <td>(hildings, 1.0)</td>\n",
       "      <td>(throw, 0.927)</td>\n",
       "      <td>(physic, 0.8825)</td>\n",
       "      <td>(hell, 0.8918)</td>\n",
       "      <td>(them, 0.9791)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(puffs, 1.0)</td>\n",
       "      <td>(stakes, 1.0)</td>\n",
       "      <td>(sorrow, 0.9238)</td>\n",
       "      <td>(quoted, 0.8825)</td>\n",
       "      <td>(distemper, 0.8911)</td>\n",
       "      <td>(they, 0.975)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(behests, 1.0)</td>\n",
       "      <td>(heareth, 1.0)</td>\n",
       "      <td>(kingly, 0.9174)</td>\n",
       "      <td>(breach, 0.8825)</td>\n",
       "      <td>(ambition, 0.8858)</td>\n",
       "      <td>(fellow, 0.9729)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(stakes, 1.0)</td>\n",
       "      <td>(earthen, 1.0)</td>\n",
       "      <td>(glory, 0.917)</td>\n",
       "      <td>(german, 0.8825)</td>\n",
       "      <td>(effect, 0.8813)</td>\n",
       "      <td>(than, 0.9712)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                juliet              romeo                royal  \\\n",
       "rank                                                             \n",
       "1      (displant, 1.0)    (gossamer, 1.0)  (sovereign, 0.9676)   \n",
       "2      (heretics, 1.0)       (dowdy, 1.0)     (forego, 0.9416)   \n",
       "3        (shapen, 1.0)     (rooteth, 1.0)      (boast, 0.9395)   \n",
       "4       (hopeful, 1.0)  (engrossing, 1.0)      (crown, 0.9385)   \n",
       "5         (tilts, 1.0)     (scaring, 1.0)   (highness, 0.9354)   \n",
       "6         (vowel, 1.0)  (carelessly, 1.0)    (burthen, 0.9301)   \n",
       "7     (grievance, 1.0)    (hildings, 1.0)       (throw, 0.927)   \n",
       "8         (puffs, 1.0)      (stakes, 1.0)     (sorrow, 0.9238)   \n",
       "9       (behests, 1.0)     (heareth, 1.0)     (kingly, 0.9174)   \n",
       "10       (stakes, 1.0)     (earthen, 1.0)       (glory, 0.917)   \n",
       "\n",
       "                      evil                wicked                he  \n",
       "rank                                                                \n",
       "1            (suit, 0.922)     (offence, 0.9118)     (him, 0.9928)  \n",
       "2           (beam, 0.9152)    (question, 0.9032)    (have, 0.9894)  \n",
       "3     (employment, 0.9058)  (conscience, 0.9003)     (had, 0.9886)  \n",
       "4        (foolery, 0.9014)       (image, 0.8997)  (cannot, 0.9874)  \n",
       "5          (object, 0.887)         (top, 0.8986)    (would, 0.986)  \n",
       "6           (rags, 0.8825)       (gifts, 0.8943)   (known, 0.9796)  \n",
       "7         (physic, 0.8825)        (hell, 0.8918)    (them, 0.9791)  \n",
       "8         (quoted, 0.8825)   (distemper, 0.8911)     (they, 0.975)  \n",
       "9         (breach, 0.8825)    (ambition, 0.8858)  (fellow, 0.9729)  \n",
       "10        (german, 0.8825)      (effect, 0.8813)    (than, 0.9712)  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\n",
    "    '\\nThe 10 most similar words to \"%s\" using cosine-similarity on term-document frequency matrix are:'\n",
    "    % (words)\n",
    ")\n",
    "get_ranks(td_matrix, words, vocab, vocab_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: In our term-document matrix, the rows are word vectors of  dimensions. Do you think thatâ€™s enough to represent the meaning of words?\n",
    "\n",
    "I do not think that is enough to represent a good approximation of the meaning of words. Clearly document frequency, regardless of the number of documents, would not be able to cover every caveat of the English language, particularly Shakespearean language where words are frequently being made up. However, we can see that for simple words such as \"royal\" that it is able to select generally similar words. In the case of a proper noun such as 'juliet' or 'romeo', 12 documents is not enough to represent similar words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-Context Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 10 most similar words to \"['juliet', 'romeo', 'royal', 'evil', 'wicked', 'he']\" using cosine-similarity on term-context frequency matrix are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>juliet</th>\n",
       "      <th>romeo</th>\n",
       "      <th>royal</th>\n",
       "      <th>evil</th>\n",
       "      <th>wicked</th>\n",
       "      <th>he</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(helena, 0.7085)</td>\n",
       "      <td>(she, 0.7231)</td>\n",
       "      <td>(presence, 0.7248)</td>\n",
       "      <td>(only, 0.5112)</td>\n",
       "      <td>(most, 0.6687)</td>\n",
       "      <td>(she, 0.9206)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(therefore, 0.6935)</td>\n",
       "      <td>(dead, 0.6864)</td>\n",
       "      <td>(hand, 0.7151)</td>\n",
       "      <td>(this, 0.4996)</td>\n",
       "      <td>(wall, 0.661)</td>\n",
       "      <td>(it, 0.8611)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(nurse, 0.6871)</td>\n",
       "      <td>(there, 0.6861)</td>\n",
       "      <td>(head, 0.6831)</td>\n",
       "      <td>(which, 0.4959)</td>\n",
       "      <td>(great, 0.6495)</td>\n",
       "      <td>(who, 0.7644)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(demetrius, 0.6843)</td>\n",
       "      <td>(here, 0.6851)</td>\n",
       "      <td>(honour, 0.6822)</td>\n",
       "      <td>(t, 0.49)</td>\n",
       "      <td>(such, 0.642)</td>\n",
       "      <td>(there, 0.7499)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(bertram, 0.681)</td>\n",
       "      <td>(he, 0.6815)</td>\n",
       "      <td>(state, 0.6804)</td>\n",
       "      <td>(loves, 0.4854)</td>\n",
       "      <td>(sweet, 0.6391)</td>\n",
       "      <td>(so, 0.7484)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(costard, 0.6781)</td>\n",
       "      <td>(tybalt, 0.6634)</td>\n",
       "      <td>(sight, 0.6795)</td>\n",
       "      <td>(but, 0.4851)</td>\n",
       "      <td>(pit, 0.6332)</td>\n",
       "      <td>(that, 0.7287)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(falstaff, 0.6771)</td>\n",
       "      <td>(caesar, 0.6598)</td>\n",
       "      <td>(blood, 0.6755)</td>\n",
       "      <td>(aught, 0.4834)</td>\n",
       "      <td>(fair, 0.6249)</td>\n",
       "      <td>(dead, 0.7256)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(caesar, 0.676)</td>\n",
       "      <td>(lucentio, 0.6394)</td>\n",
       "      <td>(fair, 0.6754)</td>\n",
       "      <td>(something, 0.4806)</td>\n",
       "      <td>(woodcock, 0.6233)</td>\n",
       "      <td>(this, 0.7239)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(her, 0.6709)</td>\n",
       "      <td>(juliet, 0.6382)</td>\n",
       "      <td>(great, 0.6713)</td>\n",
       "      <td>(himself, 0.4801)</td>\n",
       "      <td>(commoner, 0.6228)</td>\n",
       "      <td>(which, 0.7124)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(others, 0.6697)</td>\n",
       "      <td>(hamlet, 0.6164)</td>\n",
       "      <td>(company, 0.6693)</td>\n",
       "      <td>(has, 0.4794)</td>\n",
       "      <td>(form, 0.6228)</td>\n",
       "      <td>(indeed, 0.7111)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   juliet               romeo               royal  \\\n",
       "rank                                                                \n",
       "1        (helena, 0.7085)       (she, 0.7231)  (presence, 0.7248)   \n",
       "2     (therefore, 0.6935)      (dead, 0.6864)      (hand, 0.7151)   \n",
       "3         (nurse, 0.6871)     (there, 0.6861)      (head, 0.6831)   \n",
       "4     (demetrius, 0.6843)      (here, 0.6851)    (honour, 0.6822)   \n",
       "5        (bertram, 0.681)        (he, 0.6815)     (state, 0.6804)   \n",
       "6       (costard, 0.6781)    (tybalt, 0.6634)     (sight, 0.6795)   \n",
       "7      (falstaff, 0.6771)    (caesar, 0.6598)     (blood, 0.6755)   \n",
       "8         (caesar, 0.676)  (lucentio, 0.6394)      (fair, 0.6754)   \n",
       "9           (her, 0.6709)    (juliet, 0.6382)     (great, 0.6713)   \n",
       "10       (others, 0.6697)    (hamlet, 0.6164)   (company, 0.6693)   \n",
       "\n",
       "                     evil              wicked                he  \n",
       "rank                                                             \n",
       "1          (only, 0.5112)      (most, 0.6687)     (she, 0.9206)  \n",
       "2          (this, 0.4996)       (wall, 0.661)      (it, 0.8611)  \n",
       "3         (which, 0.4959)     (great, 0.6495)     (who, 0.7644)  \n",
       "4               (t, 0.49)       (such, 0.642)   (there, 0.7499)  \n",
       "5         (loves, 0.4854)     (sweet, 0.6391)      (so, 0.7484)  \n",
       "6           (but, 0.4851)       (pit, 0.6332)    (that, 0.7287)  \n",
       "7         (aught, 0.4834)      (fair, 0.6249)    (dead, 0.7256)  \n",
       "8     (something, 0.4806)  (woodcock, 0.6233)    (this, 0.7239)  \n",
       "9       (himself, 0.4801)  (commoner, 0.6228)   (which, 0.7124)  \n",
       "10          (has, 0.4794)      (form, 0.6228)  (indeed, 0.7111)  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\n",
    "    '\\nThe 10 most similar words to \"%s\" using cosine-similarity on term-context frequency matrix are:'\n",
    "    % (words)\n",
    ")\n",
    "get_ranks(tc_matrix, words, vocab, vocab_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regard to the more accurate term-context matrix, we can see that the term-context matrix does slightly better at finding similar words to proper nouns. Juliet's most similar word is compared to helena which is another fictional character involved in a complicated love story. It is slightly surprising Romeo's top word is \"she\"; however, in Shakespeare's play he was quite the romantic poet, so it is unsurprising his name would co-occur with \"she\" somewhat frequently. Looking at the non-proper nouns, the word 'royal' has decently similar words in both representations. In the term-document matrix, words such as 'sovereign', 'crown', and 'highness' are words which I would consider to be very similar to 'royal'. However, these do not appear in the term-context matrix. That being said, the term-context matrix finds other words which are still similar but in a slightly different sense. Royal in the term-context is more similar to words which represent ruling or governing postitions such as \"head\", \"state\", and \"presence\". \n",
    "\n",
    "Both representations seem to struggle with representing \"evil\" where neither seemed to find words which I would truly consider to be similar to one another.\n",
    "\n",
    "Overall, both vector representations have their issues; however, it is difficult to say whether one is objectively better than the other. Perhaps with more documents these would be different; however, depending on the type of similarity a person is looking for, one method of representation may be preferred over the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To elaborate on the previous question of whether words can self occur, we can get the ranks using the \"bad\" term-context matrix as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>juliet</th>\n",
       "      <th>romeo</th>\n",
       "      <th>royal</th>\n",
       "      <th>evil</th>\n",
       "      <th>wicked</th>\n",
       "      <th>he</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(pined, 0.5597)</td>\n",
       "      <td>(retorts, 0.5655)</td>\n",
       "      <td>(couplement, 0.5771)</td>\n",
       "      <td>(excels, 0.3483)</td>\n",
       "      <td>(converts, 0.3434)</td>\n",
       "      <td>(heareth, 0.8155)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(waken, 0.4198)</td>\n",
       "      <td>(jour, 0.497)</td>\n",
       "      <td>(chasing, 0.4883)</td>\n",
       "      <td>(is, 0.2083)</td>\n",
       "      <td>(dissembling, 0.3235)</td>\n",
       "      <td>(capers, 0.7866)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(and, 0.2526)</td>\n",
       "      <td>(hist, 0.4324)</td>\n",
       "      <td>(presences, 0.4883)</td>\n",
       "      <td>(this, 0.2029)</td>\n",
       "      <td>(commoner, 0.2873)</td>\n",
       "      <td>(numbered, 0.7407)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(drinkings, 0.2021)</td>\n",
       "      <td>(snatching, 0.3885)</td>\n",
       "      <td>(selves, 0.4078)</td>\n",
       "      <td>(he, 0.1994)</td>\n",
       "      <td>(corpulent, 0.2752)</td>\n",
       "      <td>(parallels, 0.7286)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(scamble, 0.1999)</td>\n",
       "      <td>(lengthens, 0.3729)</td>\n",
       "      <td>(occupation, 0.3987)</td>\n",
       "      <td>(unbuckles, 0.1947)</td>\n",
       "      <td>(copatain, 0.2682)</td>\n",
       "      <td>(stirreth, 0.6895)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(metheglins, 0.1999)</td>\n",
       "      <td>(exiled, 0.3415)</td>\n",
       "      <td>(trumpeters, 0.3844)</td>\n",
       "      <td>(trusts, 0.1947)</td>\n",
       "      <td>(a, 0.2635)</td>\n",
       "      <td>(nill, 0.646)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(wills, 0.1941)</td>\n",
       "      <td>(rosemary, 0.2875)</td>\n",
       "      <td>(fronts, 0.3332)</td>\n",
       "      <td>(recorded, 0.1947)</td>\n",
       "      <td>(o, 0.2614)</td>\n",
       "      <td>(replenished, 0.6291)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(bleeding, 0.1933)</td>\n",
       "      <td>(slew, 0.2864)</td>\n",
       "      <td>(bail, 0.3206)</td>\n",
       "      <td>(default, 0.1947)</td>\n",
       "      <td>(fiend, 0.2573)</td>\n",
       "      <td>(covetous, 0.6291)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(wakes, 0.1871)</td>\n",
       "      <td>(banished, 0.2321)</td>\n",
       "      <td>(empress, 0.247)</td>\n",
       "      <td>(drivelling, 0.1947)</td>\n",
       "      <td>(charitable, 0.2538)</td>\n",
       "      <td>(tilts, 0.6044)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(enter, 0.186)</td>\n",
       "      <td>(doff, 0.2143)</td>\n",
       "      <td>(lists, 0.2177)</td>\n",
       "      <td>(abhominable, 0.1947)</td>\n",
       "      <td>(combination, 0.2448)</td>\n",
       "      <td>(enjoys, 0.6044)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    juliet                romeo                 royal  \\\n",
       "rank                                                                    \n",
       "1          (pined, 0.5597)    (retorts, 0.5655)  (couplement, 0.5771)   \n",
       "2          (waken, 0.4198)        (jour, 0.497)     (chasing, 0.4883)   \n",
       "3            (and, 0.2526)       (hist, 0.4324)   (presences, 0.4883)   \n",
       "4      (drinkings, 0.2021)  (snatching, 0.3885)      (selves, 0.4078)   \n",
       "5        (scamble, 0.1999)  (lengthens, 0.3729)  (occupation, 0.3987)   \n",
       "6     (metheglins, 0.1999)     (exiled, 0.3415)  (trumpeters, 0.3844)   \n",
       "7          (wills, 0.1941)   (rosemary, 0.2875)      (fronts, 0.3332)   \n",
       "8       (bleeding, 0.1933)       (slew, 0.2864)        (bail, 0.3206)   \n",
       "9          (wakes, 0.1871)   (banished, 0.2321)      (empress, 0.247)   \n",
       "10          (enter, 0.186)       (doff, 0.2143)       (lists, 0.2177)   \n",
       "\n",
       "                       evil                 wicked                     he  \n",
       "rank                                                                       \n",
       "1          (excels, 0.3483)     (converts, 0.3434)      (heareth, 0.8155)  \n",
       "2              (is, 0.2083)  (dissembling, 0.3235)       (capers, 0.7866)  \n",
       "3            (this, 0.2029)     (commoner, 0.2873)     (numbered, 0.7407)  \n",
       "4              (he, 0.1994)    (corpulent, 0.2752)    (parallels, 0.7286)  \n",
       "5       (unbuckles, 0.1947)     (copatain, 0.2682)     (stirreth, 0.6895)  \n",
       "6          (trusts, 0.1947)            (a, 0.2635)          (nill, 0.646)  \n",
       "7        (recorded, 0.1947)            (o, 0.2614)  (replenished, 0.6291)  \n",
       "8         (default, 0.1947)        (fiend, 0.2573)     (covetous, 0.6291)  \n",
       "9      (drivelling, 0.1947)   (charitable, 0.2538)        (tilts, 0.6044)  \n",
       "10    (abhominable, 0.1947)  (combination, 0.2448)       (enjoys, 0.6044)  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ranks(bad_tc, words, vocab, vocab_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see again that the most similar words do not seem to make much sense. It was best then to use a matrix which does not allow words to occur with themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency Inverse Document Frequency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 10 most similar words to \"['juliet', 'romeo', 'royal', 'evil', 'wicked', 'he']\" using cosine-similarity on term-context frequency matrix are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>juliet</th>\n",
       "      <th>romeo</th>\n",
       "      <th>royal</th>\n",
       "      <th>evil</th>\n",
       "      <th>wicked</th>\n",
       "      <th>he</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(gadding, 1.0)</td>\n",
       "      <td>(worshipp, 1.0)</td>\n",
       "      <td>(confound, 0.9622)</td>\n",
       "      <td>(beam, 0.9656)</td>\n",
       "      <td>(owner, 0.9214)</td>\n",
       "      <td>(worthy, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(prevails, 1.0)</td>\n",
       "      <td>(scathe, 1.0)</td>\n",
       "      <td>(highness, 0.9533)</td>\n",
       "      <td>(physic, 0.9124)</td>\n",
       "      <td>(damned, 0.9063)</td>\n",
       "      <td>(clearness, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(advances, 1.0)</td>\n",
       "      <td>(blubbering, 1.0)</td>\n",
       "      <td>(sovereign, 0.9495)</td>\n",
       "      <td>(shoot, 0.9048)</td>\n",
       "      <td>(hole, 0.9005)</td>\n",
       "      <td>(meet, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(collar, 1.0)</td>\n",
       "      <td>(cheering, 1.0)</td>\n",
       "      <td>(gracious, 0.9445)</td>\n",
       "      <td>(foolery, 0.9016)</td>\n",
       "      <td>(step, 0.8971)</td>\n",
       "      <td>(pain, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(festering, 1.0)</td>\n",
       "      <td>(easter, 1.0)</td>\n",
       "      <td>(kingly, 0.9423)</td>\n",
       "      <td>(suit, 0.8957)</td>\n",
       "      <td>(everlasting, 0.8969)</td>\n",
       "      <td>(pruning, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(benedicite, 1.0)</td>\n",
       "      <td>(angelica, 1.0)</td>\n",
       "      <td>(crown, 0.9376)</td>\n",
       "      <td>(employment, 0.8956)</td>\n",
       "      <td>(start, 0.8822)</td>\n",
       "      <td>(dainty, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(cherishing, 1.0)</td>\n",
       "      <td>(riddling, 1.0)</td>\n",
       "      <td>(ends, 0.935)</td>\n",
       "      <td>(meed, 0.894)</td>\n",
       "      <td>(manners, 0.8791)</td>\n",
       "      <td>(reproach, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(absolved, 1.0)</td>\n",
       "      <td>(hoarse, 1.0)</td>\n",
       "      <td>(majesty, 0.9308)</td>\n",
       "      <td>(object, 0.8784)</td>\n",
       "      <td>(cursed, 0.8766)</td>\n",
       "      <td>(buds, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(heartless, 1.0)</td>\n",
       "      <td>(perjuries, 1.0)</td>\n",
       "      <td>(superfluous, 0.9228)</td>\n",
       "      <td>(rags, 0.8706)</td>\n",
       "      <td>(fee, 0.8763)</td>\n",
       "      <td>(slily, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(poultice, 1.0)</td>\n",
       "      <td>(usest, 1.0)</td>\n",
       "      <td>(war, 0.9132)</td>\n",
       "      <td>(behaviors, 0.8706)</td>\n",
       "      <td>(laid, 0.8755)</td>\n",
       "      <td>(william, 0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 juliet              romeo                  royal  \\\n",
       "rank                                                                \n",
       "1        (gadding, 1.0)    (worshipp, 1.0)     (confound, 0.9622)   \n",
       "2       (prevails, 1.0)      (scathe, 1.0)     (highness, 0.9533)   \n",
       "3       (advances, 1.0)  (blubbering, 1.0)    (sovereign, 0.9495)   \n",
       "4         (collar, 1.0)    (cheering, 1.0)     (gracious, 0.9445)   \n",
       "5      (festering, 1.0)      (easter, 1.0)       (kingly, 0.9423)   \n",
       "6     (benedicite, 1.0)    (angelica, 1.0)        (crown, 0.9376)   \n",
       "7     (cherishing, 1.0)    (riddling, 1.0)          (ends, 0.935)   \n",
       "8       (absolved, 1.0)      (hoarse, 1.0)      (majesty, 0.9308)   \n",
       "9      (heartless, 1.0)   (perjuries, 1.0)  (superfluous, 0.9228)   \n",
       "10      (poultice, 1.0)       (usest, 1.0)          (war, 0.9132)   \n",
       "\n",
       "                      evil                 wicked              he  \n",
       "rank                                                               \n",
       "1           (beam, 0.9656)        (owner, 0.9214)     (worthy, 0)  \n",
       "2         (physic, 0.9124)       (damned, 0.9063)  (clearness, 0)  \n",
       "3          (shoot, 0.9048)         (hole, 0.9005)       (meet, 0)  \n",
       "4        (foolery, 0.9016)         (step, 0.8971)       (pain, 0)  \n",
       "5           (suit, 0.8957)  (everlasting, 0.8969)    (pruning, 0)  \n",
       "6     (employment, 0.8956)        (start, 0.8822)     (dainty, 0)  \n",
       "7            (meed, 0.894)      (manners, 0.8791)   (reproach, 0)  \n",
       "8         (object, 0.8784)       (cursed, 0.8766)       (buds, 0)  \n",
       "9           (rags, 0.8706)          (fee, 0.8763)      (slily, 0)  \n",
       "10     (behaviors, 0.8706)         (laid, 0.8755)    (william, 0)  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\n",
    "    '\\nThe 10 most similar words to \"%s\" using cosine-similarity on term-context frequency matrix are:'\n",
    "    % (words)\n",
    ")\n",
    "get_ranks(tf_idf_matrix, words, vocab, vocab_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive Pointwise Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 10 most similar words to \"['juliet', 'romeo', 'royal', 'evil', 'wicked', 'he']\" using cosine-similarity on term-context frequency matrix are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>juliet</th>\n",
       "      <th>romeo</th>\n",
       "      <th>royal</th>\n",
       "      <th>evil</th>\n",
       "      <th>wicked</th>\n",
       "      <th>he</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(capulet, 0.1819)</td>\n",
       "      <td>(mercutio, 0.2296)</td>\n",
       "      <td>(encountering, 0.1827)</td>\n",
       "      <td>(discoveries, 0.3079)</td>\n",
       "      <td>(spectacle, 0.2403)</td>\n",
       "      <td>(hath, 0.1525)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(tybalt, 0.1721)</td>\n",
       "      <td>(hist, 0.2165)</td>\n",
       "      <td>(carters, 0.1406)</td>\n",
       "      <td>(ministering, 0.2747)</td>\n",
       "      <td>(bombast, 0.2196)</td>\n",
       "      <td>(that, 0.1416)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(katarina, 0.1595)</td>\n",
       "      <td>(bon, 0.1892)</td>\n",
       "      <td>(dismantled, 0.1226)</td>\n",
       "      <td>(clerks, 0.2669)</td>\n",
       "      <td>(absurd, 0.1914)</td>\n",
       "      <td>(it, 0.1257)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(paris, 0.1534)</td>\n",
       "      <td>(jour, 0.1836)</td>\n",
       "      <td>(peril, 0.1184)</td>\n",
       "      <td>(crawl, 0.2594)</td>\n",
       "      <td>(disperse, 0.1841)</td>\n",
       "      <td>(is, 0.1251)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(executioners, 0.1521)</td>\n",
       "      <td>(booted, 0.1673)</td>\n",
       "      <td>(prescription, 0.1175)</td>\n",
       "      <td>(interrupt, 0.25)</td>\n",
       "      <td>(thwarted, 0.1778)</td>\n",
       "      <td>(when, 0.1185)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(spit, 0.1503)</td>\n",
       "      <td>(kinsman, 0.1543)</td>\n",
       "      <td>(itches, 0.1173)</td>\n",
       "      <td>(unfit, 0.2277)</td>\n",
       "      <td>(angelical, 0.1754)</td>\n",
       "      <td>(she, 0.1136)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(demurely, 0.1469)</td>\n",
       "      <td>(tybalt, 0.1538)</td>\n",
       "      <td>(commended, 0.1166)</td>\n",
       "      <td>(unseemly, 0.2229)</td>\n",
       "      <td>(diverted, 0.1718)</td>\n",
       "      <td>(him, 0.1105)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(stabbed, 0.1466)</td>\n",
       "      <td>(slew, 0.1457)</td>\n",
       "      <td>(detestable, 0.1144)</td>\n",
       "      <td>(hunting, 0.2194)</td>\n",
       "      <td>(cools, 0.1652)</td>\n",
       "      <td>(me, 0.1105)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(rosencrantz, 0.1447)</td>\n",
       "      <td>(pined, 0.145)</td>\n",
       "      <td>(usurped, 0.114)</td>\n",
       "      <td>(conspires, 0.2166)</td>\n",
       "      <td>(resolutely, 0.1626)</td>\n",
       "      <td>(his, 0.1099)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(behove, 0.1437)</td>\n",
       "      <td>(friar, 0.1343)</td>\n",
       "      <td>(maculate, 0.1124)</td>\n",
       "      <td>(brine, 0.2037)</td>\n",
       "      <td>(obscene, 0.1574)</td>\n",
       "      <td>(himself, 0.1022)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      juliet               romeo                   royal  \\\n",
       "rank                                                                       \n",
       "1          (capulet, 0.1819)  (mercutio, 0.2296)  (encountering, 0.1827)   \n",
       "2           (tybalt, 0.1721)      (hist, 0.2165)       (carters, 0.1406)   \n",
       "3         (katarina, 0.1595)       (bon, 0.1892)    (dismantled, 0.1226)   \n",
       "4            (paris, 0.1534)      (jour, 0.1836)         (peril, 0.1184)   \n",
       "5     (executioners, 0.1521)    (booted, 0.1673)  (prescription, 0.1175)   \n",
       "6             (spit, 0.1503)   (kinsman, 0.1543)        (itches, 0.1173)   \n",
       "7         (demurely, 0.1469)    (tybalt, 0.1538)     (commended, 0.1166)   \n",
       "8          (stabbed, 0.1466)      (slew, 0.1457)    (detestable, 0.1144)   \n",
       "9      (rosencrantz, 0.1447)      (pined, 0.145)        (usurped, 0.114)   \n",
       "10          (behove, 0.1437)     (friar, 0.1343)      (maculate, 0.1124)   \n",
       "\n",
       "                       evil                wicked                 he  \n",
       "rank                                                                  \n",
       "1     (discoveries, 0.3079)   (spectacle, 0.2403)     (hath, 0.1525)  \n",
       "2     (ministering, 0.2747)     (bombast, 0.2196)     (that, 0.1416)  \n",
       "3          (clerks, 0.2669)      (absurd, 0.1914)       (it, 0.1257)  \n",
       "4           (crawl, 0.2594)    (disperse, 0.1841)       (is, 0.1251)  \n",
       "5         (interrupt, 0.25)    (thwarted, 0.1778)     (when, 0.1185)  \n",
       "6           (unfit, 0.2277)   (angelical, 0.1754)      (she, 0.1136)  \n",
       "7        (unseemly, 0.2229)    (diverted, 0.1718)      (him, 0.1105)  \n",
       "8         (hunting, 0.2194)       (cools, 0.1652)       (me, 0.1105)  \n",
       "9       (conspires, 0.2166)  (resolutely, 0.1626)      (his, 0.1099)  \n",
       "10          (brine, 0.2037)     (obscene, 0.1574)  (himself, 0.1022)  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\n",
    "    '\\nThe 10 most similar words to \"%s\" using cosine-similarity on term-context frequency matrix are:'\n",
    "    % (words)\n",
    ")\n",
    "get_ranks(ppmi_matrix, words, vocab, vocab_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Extra Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>POS</th>\n",
       "      <th>SimLex999</th>\n",
       "      <th>conc(w1)</th>\n",
       "      <th>conc(w2)</th>\n",
       "      <th>concQ</th>\n",
       "      <th>Assoc(USF)</th>\n",
       "      <th>SimAssoc333</th>\n",
       "      <th>SD(SimLex)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>old</td>\n",
       "      <td>new</td>\n",
       "      <td>A</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smart</td>\n",
       "      <td>intelligent</td>\n",
       "      <td>A</td>\n",
       "      <td>9.20</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard</td>\n",
       "      <td>difficult</td>\n",
       "      <td>A</td>\n",
       "      <td>8.77</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2</td>\n",
       "      <td>5.94</td>\n",
       "      <td>1</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>cheerful</td>\n",
       "      <td>A</td>\n",
       "      <td>9.55</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1</td>\n",
       "      <td>5.85</td>\n",
       "      <td>1</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hard</td>\n",
       "      <td>easy</td>\n",
       "      <td>A</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.07</td>\n",
       "      <td>2</td>\n",
       "      <td>5.82</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word1        word2 POS  SimLex999  conc(w1)  conc(w2)  concQ  Assoc(USF)  \\\n",
       "0    old          new   A       1.58      2.72      2.81      2        7.25   \n",
       "1  smart  intelligent   A       9.20      1.75      2.46      1        7.11   \n",
       "2   hard    difficult   A       8.77      3.76      2.21      2        5.94   \n",
       "3  happy     cheerful   A       9.55      2.56      2.34      1        5.85   \n",
       "4   hard         easy   A       0.95      3.76      2.07      2        5.82   \n",
       "\n",
       "   SimAssoc333  SD(SimLex)  \n",
       "0            1        0.41  \n",
       "1            1        0.67  \n",
       "2            1        1.19  \n",
       "3            1        2.18  \n",
       "4            1        0.93  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simlex = pd.read_csv('./SimLex-999/SimLex-999.txt', sep = '\\t')\n",
    "simlex.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are primarily interested in the SimLex999 column which are the human annotated scores on the scale from [0,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simlex.shape = (999, 10)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{simlex.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = {\n",
    "    'words' : [],\n",
    "    'Term-Document' : [],\n",
    "    'Term-Context' : [],\n",
    "    'TF-IDF' : [],\n",
    "    'PPMI' : [],\n",
    "    'simlex' : []\n",
    "}\n",
    "missed = []\n",
    "\n",
    "for row in range(simlex.shape[0]):\n",
    "    word1 = simlex['word1'][row]\n",
    "    word2 = simlex['word2'][row]\n",
    "    if (word1 not in vocab) or (word2 not in vocab):\n",
    "        missed.append((word1,word2))\n",
    "        continue\n",
    "\n",
    "    w1i = vocab_to_index[word1]\n",
    "    w2i = vocab_to_index[word2]\n",
    "\n",
    "    sims['words'].append((word1,word2))\n",
    "    sims['Term-Document'].append(compute_cosine_similarity(td_matrix[w1i,:],td_matrix[w2i,:]))\n",
    "    sims['Term-Context'].append(compute_cosine_similarity(tc_matrix[w1i,:],tc_matrix[w2i,:]))\n",
    "    sims['TF-IDF'].append(compute_cosine_similarity(tf_idf_matrix[w1i,:],tf_idf_matrix[w2i,:]))\n",
    "    sims['PPMI'].append(compute_cosine_similarity(ppmi_matrix[w1i,:],ppmi_matrix[w2i,:]))\n",
    "    sims['simlex'].append(simlex['SimLex999'][row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H0 : tau = 0\n",
      "H1 : tau != 0\n",
      "\n",
      "Corr. between Cosine Similarity with Term-Document and SimLex999 Human Annotations = -0.10189564170793766\n",
      "p-value = 0.000297\n",
      "\n",
      "Corr. between Cosine Similarity with Term-Context and SimLex999 Human Annotations = -0.07814316791152155\n",
      "p-value = 0.005397\n",
      "\n",
      "Corr. between Cosine Similarity with TF-IDF and SimLex999 Human Annotations = -0.03419435165347337\n",
      "p-value = 0.255945\n",
      "\n",
      "Corr. between Cosine Similarity with PPMI and SimLex999 Human Annotations = -0.024001290161916693\n",
      "p-value = 0.393026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"H0 : tau = 0\")\n",
    "print(f\"H1 : tau != 0\\n\")\n",
    "\n",
    "for k,v in sims.items():\n",
    "    if k in ['words', 'simlex']: continue\n",
    "\n",
    "    kt = kendalltau(v, sims['simlex'], alternative='two-sided')\n",
    "    print(f\"Corr. between Cosine Similarity with {k} and SimLex999 Human Annotations = {kt.statistic}\")\n",
    "    print(f\"p-value = {round(kt.pvalue,6)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
