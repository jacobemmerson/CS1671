{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS1671 Assignment 4: Vector Space Models\n",
    "### Jacob Emmerson\n",
    "Due: Novemeber 20th, 2023 @ 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment primarily focuses arround the material from chapter 6 in *Speech and Language Processing* (3rd Ed.)\n",
    "\n",
    "**Primary Question:** How good are vector space representations built using Shakespeare data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hw4_skeleton_jte27 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://numpy.org/doc/stable/reference/routines.array-creation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_shakespeare():\n",
    "    \"\"\"Reads in the Shakespeare dataset processesit into a list of tuples.\n",
    "       Also reads in the vocab and play name lists from files.\n",
    "\n",
    "    Each tuple consists of\n",
    "    tuple[0]: The name of the play\n",
    "    tuple[1] A line from the play as a list of tokenized words.\n",
    "\n",
    "    Returns:\n",
    "      tuples: A list of tuples in the above format.\n",
    "      document_names: A list of the plays present in the corpus.\n",
    "      vocab: A list of all tokens in the vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    tuples = []\n",
    "\n",
    "    with open(\"will_play_text.csv\") as f:\n",
    "        csv_reader = csv.reader(f, delimiter=\";\")\n",
    "        for row in csv_reader:\n",
    "            play_name = row[1]\n",
    "            line = row[5]\n",
    "            line_tokens = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", line).split()\n",
    "            line_tokens = [token.lower() for token in line_tokens]\n",
    "\n",
    "            tuples.append((play_name, line_tokens))\n",
    "\n",
    "    with open(\"vocab.txt\") as f:\n",
    "        vocab = [line.strip() for line in f]\n",
    "\n",
    "    with open(\"play_names.txt\") as f:\n",
    "        document_names = [line.strip() for line in f]\n",
    "\n",
    "    return tuples, document_names, vocab\n",
    "\n",
    "\n",
    "def get_row_vector(matrix, row_id):\n",
    "    \"\"\"A convenience function to get a particular row vector from a numpy matrix\n",
    "\n",
    "    Inputs:\n",
    "      matrix: a 2-dimensional numpy array\n",
    "      row_id: an integer row_index for the desired row vector\n",
    "\n",
    "    Returns:\n",
    "      1-dimensional numpy array of the row vector\n",
    "    \"\"\"\n",
    "    return matrix[row_id, :]\n",
    "\n",
    "\n",
    "def get_column_vector(matrix, col_id):\n",
    "    \"\"\"A convenience function to get a particular column vector from a numpy matrix\n",
    "\n",
    "    Inputs:\n",
    "      matrix: a 2-dimensional numpy array\n",
    "      col_id: an integer col_index for the desired row vector\n",
    "\n",
    "    Returns:\n",
    "      1-dimensional numpy array of the column vector\n",
    "    \"\"\"\n",
    "    return matrix[:, col_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_document_matrix(line_tuples, document_names, vocab):\n",
    "    \"\"\"Returns a numpy array containing the term document matrix for the input lines.\n",
    "\n",
    "    Inputs:\n",
    "      line_tuples: A list of tuples, containing the name of the document and\n",
    "      a tokenized line from that document.\n",
    "      document_names: A list of the document names\n",
    "      vocab: A list of the tokens in the vocabulary\n",
    "\n",
    "    # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:39 PM.\n",
    "\n",
    "    Let m = len(vocab) and n = len(document_names).\n",
    "\n",
    "    Returns:\n",
    "      td_matrix: A mxn numpy array where the number of rows is the number of words\n",
    "          and each column corresponds to a document. A_ij contains the\n",
    "          frequency with which word i occurs in document j.\n",
    "    \"\"\"\n",
    "    m = len(vocab)\n",
    "    n = len(document_names)\n",
    "\n",
    "    doc_index = dict(zip(document_names, range(len(document_names))))\n",
    "    word_index = dict(zip(vocab, range(len(vocab))))\n",
    "    td_matrix = np.zeros(shape = (n,m), dtype=np.int32)\n",
    "\n",
    "    for line in line_tuples:\n",
    "        di = doc_index[line[0]]\n",
    "        words = line[1]\n",
    "        for w in words:\n",
    "            wi = word_index[w]\n",
    "            td_matrix[di, wi] += 1\n",
    "\n",
    "    return td_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_context_matrix(line_tuples, vocab, context_window_size=1):\n",
    "    \"\"\"Returns a numpy array containing the term context matrix for the input lines.\n",
    "\n",
    "    Inputs:\n",
    "      line_tuples: A list of tuples, containing the name of the document and\n",
    "      a tokenized line from that document.\n",
    "      vocab: A list of the tokens in the vocabulary\n",
    "\n",
    "    # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:39 PM.\n",
    "\n",
    "    Let n = len(vocab).\n",
    "\n",
    "    Returns:\n",
    "      tc_matrix: A nxn numpy array where A_ij contains the frequency with which\n",
    "          word j was found within context_window_size to the left or right of\n",
    "          word i in any sentence in the tuples.\n",
    "    \"\"\"\n",
    "    n = len(vocab)\n",
    "    cws = context_window_size\n",
    "\n",
    "    word_index = dict(zip(vocab, range(len(vocab))))\n",
    "    tc_matrix = np.zeros(shape = (n,n), dtype = np.int32)\n",
    "\n",
    "    for line in line_tuples:\n",
    "        words = line[1]\n",
    "\n",
    "        for i in range(len(words)):\n",
    "            wi = word_index[words[i]]\n",
    "            L_win = words[max(0, i - cws):i] # upper is exclusive\n",
    "\n",
    "            if i == len(words): # if we are at the end of the sentence, no upper window \n",
    "              #(i + 1) throws an error\n",
    "                U_win = []\n",
    "            else: \n",
    "                U_win = words[(i+1):(i + cws + 1)]\n",
    "\n",
    "            window = L_win + U_win\n",
    "            for word in window: # add the word instances to the tc_matrix\n",
    "                wj = word_index[word]\n",
    "                tc_matrix[wi,wj] += 1\n",
    "\n",
    "    return tc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_idf_matrix(term_document_matrix):\n",
    "    \"\"\"Given the term document matrix, output a tf-idf weighted version.\n",
    "\n",
    "    See section 6.5 in the textbook.\n",
    "\n",
    "    Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "\n",
    "    Input:\n",
    "      term_document_matrix: Numpy array where each column represents a document\n",
    "      and each row, the frequency of a word in that document.\n",
    "\n",
    "    Returns:\n",
    "      A numpy array with the same dimension as term_document_matrix, where\n",
    "      A_ij is weighted by the inverse document frequency of document h.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ppmi_matrix(term_context_matrix):\n",
    "    \"\"\"Given the term context matrix, output a ppmi weighted version.\n",
    "\n",
    "    See section 6.6 in the textbook.\n",
    "\n",
    "    Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "\n",
    "    Input:\n",
    "      term_context_matrix: Numpy array where each cell represents whether the \n",
    "\t  word in the row appears within a window of the word in the column.\n",
    "\n",
    "    Returns:\n",
    "      A numpy array with the same dimension as term_context_matrix, where\n",
    "      A_ij is weighted using PPMI.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(vector1, vector2):\n",
    "    \"\"\"Computes the cosine similarity of the two input vectors.\n",
    "\n",
    "    Inputs:\n",
    "      vector1: A nx1 numpy array\n",
    "      vector2: A nx1 numpy array\n",
    "\n",
    "    Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "      A scalar similarity value.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_words(target_word_index, matrix):\n",
    "    \"\"\"Ranks the similarity of all of the words to the target word using compute_cosine_similarity.\n",
    "\n",
    "    Inputs:\n",
    "      target_word_index: The index of the word we want to compare all others against.\n",
    "      matrix: Numpy matrix where the ith row represents a vector embedding of the ith word.\n",
    "\n",
    "    Returns:\n",
    "      A length-n list of integer word indices, ordered by decreasing similarity to the\n",
    "      target word indexed by word_index\n",
    "      A length-n list of similarity scores, ordered by decreasing similarity to the\n",
    "      target word indexed by word_index\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing term document matrix...\n",
      "Computing tf-idf matrix...\n",
      "Computing term context matrix...\n",
      "act\n",
      "['i']\n",
      "i\n",
      "['act']\n",
      "\n",
      "The 10 most similar words to \"juliet\" using cosine-similarity on term-document frequency matrix are:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/kibit/pitt/CS1671/A4/writeup_jte27.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kibit/pitt/CS1671/A4/writeup_jte27.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m ranks, scores \u001b[39m=\u001b[39m rank_words(vocab_to_index[word], td_matrix)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kibit/pitt/CS1671/A4/writeup_jte27.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m10\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kibit/pitt/CS1671/A4/writeup_jte27.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     word_id \u001b[39m=\u001b[39m ranks[idx]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kibit/pitt/CS1671/A4/writeup_jte27.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m; \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m(idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, vocab[word_id], scores[idx]))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kibit/pitt/CS1671/A4/writeup_jte27.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kibit/pitt/CS1671/A4/writeup_jte27.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mThe 10 most similar words to \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m using cosine-similarity on term-context frequency matrix are:\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kibit/pitt/CS1671/A4/writeup_jte27.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m%\u001b[39m (word)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kibit/pitt/CS1671/A4/writeup_jte27.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m )\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tuples, document_names, vocab = read_in_shakespeare()\n",
    "\n",
    "    print(\"Computing term document matrix...\")\n",
    "    td_matrix = create_term_document_matrix(tuples, document_names, vocab)\n",
    "\n",
    "    print(\"Computing tf-idf matrix...\")\n",
    "    tf_idf_matrix = create_tf_idf_matrix(td_matrix)\n",
    "\n",
    "\n",
    "    print(\"Computing term context matrix...\")\n",
    "    tc_matrix = create_term_context_matrix(tuples, vocab, context_window_size=2)\n",
    "\n",
    "    # random_idx = random.randint(0, len(document_names) - 1)\n",
    "\n",
    "    word = \"juliet\"\n",
    "    vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "\n",
    "    print(\n",
    "        '\\nThe 10 most similar words to \"%s\" using cosine-similarity on term-document frequency matrix are:'\n",
    "        % (word)\n",
    "    )\n",
    "    ranks, scores = rank_words(vocab_to_index[word], td_matrix)\n",
    "    for idx in range(0,10):\n",
    "        word_id = ranks[idx]\n",
    "        print(\"%d: %s; %s\" %(idx+1, vocab[word_id], scores[idx]))\n",
    "\n",
    "    print(\n",
    "        '\\nThe 10 most similar words to \"%s\" using cosine-similarity on term-context frequency matrix are:'\n",
    "        % (word)\n",
    "    )\n",
    "    ranks, scores = rank_words(vocab_to_index[word], tc_matrix)\n",
    "    for idx in range(0,10):\n",
    "        word_id = ranks[idx]\n",
    "        print(\"%d: %s; %s\" %(idx+1, vocab[word_id], scores[idx]))\n",
    "\n",
    "\n",
    "    print(\n",
    "        '\\nThe 10 most similar words to \"%s\" using cosine-similarity on tf-idf matrix are:'\n",
    "        % (word)\n",
    "    )\n",
    "    ranks, scores = rank_words(vocab_to_index[word], tf_idf_matrix)\n",
    "    for idx in range(0,10):\n",
    "        word_id = ranks[idx]\n",
    "        print(\"%d: %s; %s\" %(idx+1, vocab[word_id], scores[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt,dn,v = read_in_shakespeare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = create_term_document_matrix(lt,dn,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = create_term_context_matrix(lt, v, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
